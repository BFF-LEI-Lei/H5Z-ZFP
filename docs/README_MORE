========
Overview
========

This H5Z-ZFP plugin was originally written for ZFP version 0.5.0.

The HDF5 filter plugin code here is part of the Silo library. However,
we have made an effort to also support it as a stand-alone package due
to the likely broad appeal and utility of the ZFP compression library.

There are potentially other implementations of HDF5 compression filter
plugins  based on  the ZFP  compression library.  Because there  are a
variety  of ways ZFP  compression metadata  can be  handled, different
implementations, if they exist, are in all likelihood incompatible.
For reference, this plugin uses the *registered* HDF5 filter ID...

    #define H5Z_FILTER_ZFP 32013

For efficiency's  sake, this plugin *does*not* embed  any ZFP metadata
*within* the  compressed stream  of any HDF5  chunk. All  relevant ZFP
control  parameters are encoded  once, for all chunks, in cd_values as
part  of the  HDF5 dataset   header.  The  same  set   of   parameters
are  used   to compress/decompress *all* chunks of a given dataset.

This  ZFP filter  will work  on any  dimension dataset  of  integer or
floating  point   data  of  either  4  or   8  bytes  (floats/doubles,
int32_t/int64_t). Although the  ZFP library works for only  1, 2 and 3
dimensional data, as  long as the dataset chunking  is such that there
are  1-3 non-unity  dimensions, this  filter will  work. The filter is
also designed to integrate well with HDF5's error reporting.



You can  use the test_write example to  generate a cd_values
array for h5repack using a command like

    test_write zfpmode=2 prec=11 --help

This  will cause  test_write  to print  the  cd_values to  be used  in
H5Pset_filter call and then exit.

The test_write.c example writes  both a compressed and UNcompressed 1D
array of samples of a single  cycle of a sinuoisal curve with optional
random noise. The test_read.c example  is a simple test to re-read the
HDF5 data and confirm the compressed data behaves as expected relative
to the uncompressed data. The command

    test_write --help

prints  a help  message  describing all  command-line arguments.  They
allow you to  control size of dataset, amount  of noise, HDF5 chunking
and ZFP filter mode and parameters. For example, the command

    test_write zfp_mode=3 acc=0.001 noise=0.1 amp=1 npoints=5000

will generate  1D array  of 5000 points  of a single  sinuoisdal cycle
with amplitude  1 and random noise  +-0.05 and then  compress it using
ZFP's accuracy mode with accuracy tolerance of 0.001.

There  are a  number  of tests  performed  in the  Makefile for  rate,
precision, accuracy  and expert mode of  the filter. As  a final test,
the filter  is used  in an h5repack  scenario where a  file containing
integer and floating point data of 1,2,3 and 4 dimensions is re-packed
using an accuracy  mode with tolerance 0.001. This  should result in >
2:1  compression.  However,  this  h5repack test  fails  with  current
versions of  HDF5 due  to a bug  in h5repack.  A patch is  required to
h5repack_parse.c. That patch is included  here if you wish to apply it
and re-build the HDF5 repack tool you are using.

The Makefile uses  GNU Make syntax and is designed to  work on OSX and
Linux. The command,

    make CC=<C-compiler> HDF5_HOME=<hdf5-dir> ZFP_HOME=<zfp-dir> all"

where hdf5-dir is  a dir containing HDF5 include,  lib and bin subdirs
and zfp-dir is a dir containing  inc and lib subdirs, should build and
test everything. The command 'make help' will print useful information
about various make targets and variables.

The filter has been tested on gcc, clang, xlc, icc and pgcc  compilers
and checked with valgrind.

When  reading  ZFP compressed  data  on  a  machine with  a  different
endian-ness    than   the   writer,    there   is    an   unnavoidable
inefficiency. ZFP  writes an endian-independent format so upon reading
it  back,  it  produces  the  correct endian-ness result regardless of
reader  and  writer  endian-ness  incompatability.  However,  the HDF5
library is expecting to read  from the file (even if through a filter)
the  endian-ness  of  the  writer  and  expects to byte-swap it before
returning  to the caller. So, in the H5Z-ZFP plugin, we wind up having
to  byte-swap  a result read in a cross-endian context. That way, when
HDF5  gets it,  it will byte-swap  again producing the correct result.
There is  an endian-ness  test in  the Makefile and two ZFP compressed
example  datasets for  big-endian  and little-endian machines to  test
that cross-endian reads/writes work correctly.

Finally, endian-targetting,  that is setting the file  datatype for an
endian-ness that is possibly  different than the native endian-ness of
the  writer,  is  currently  dis-allowed  with  H5Z-ZFP.  Under  these
conditions, the can_apply method will return 0. This constraint can be
relaxed,  at the  expense of  an additional  endian-UN-swap  pass just
prior to compression, at a future date if it becomes too onerous.
